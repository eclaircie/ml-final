{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f09300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def normalize(example):\n",
    "    # make everything lowercase \n",
    "    example = example.lower()\n",
    "    \n",
    "    # strip html tags, replace with \"\"\n",
    "    example = re.sub('\\<\\w{1,2}\\>', '', example)\n",
    "    \n",
    "    # remove punctuation; \n",
    "    #replace with keyword \" _PUNCT \" (including spaces)\n",
    "    example = re.sub('\\W', ' ', example)\n",
    "    \n",
    "    # remove links, replace with keyword \" _EXTERNALLINK \"\n",
    "    example = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)', ' EXTERNALLINK ', example)\n",
    "\n",
    "    # replace all excessive whitespace with a single space; \n",
    "    #(this will also normalize the punctuation & link counters above)\n",
    "    example = re.sub('\\s{2,}', ' ', example)\n",
    "\n",
    "    return example\n",
    "    #return void\n",
    "\n",
    "\n",
    "def process_text(filename):\n",
    "    # read 4th col of file (data) + 5th col (label)\n",
    "    strings =[]\n",
    "    labels = []\n",
    "    \n",
    "    with open (filename,'r', encoding=\"utf-8\") as csvfile:\n",
    "        csvdata = csv.reader(csvfile)\n",
    "        \n",
    "        for row in csvdata:\n",
    "            strings.append(row[3])\n",
    "            labels.append(int(row[4]))\n",
    "    \n",
    "    strings_norm = []\n",
    "    for example in strings :\n",
    "        strings_norm.append(normalize(example))\n",
    "    \n",
    "    return np.column_stack((strings_norm,labels))\n",
    "    \n",
    "\n",
    "    \n",
    "#create dictionary of most common words from an array of strings, where spam = 1\n",
    "#default dict size is 500 words\n",
    "def create_dict(arr, dict_size=500):\n",
    "    all_spam_words = [] #list of all UNIQUE spammy words\n",
    "    \n",
    "    for training_example in arr:\n",
    "        #if message is SPAM\n",
    "        if training_example[1]=='1':\n",
    "            training_string = training_example[0] #get the string\n",
    "            words = training_string.split() #split into words\n",
    "            \n",
    "            #for every word in string,\n",
    "            for word in words:\n",
    "                if not word in all_spam_words:\n",
    "                    all_spam_words.append(word) #put word in temp if its not there already\n",
    "    \n",
    "    #count occurrence of word in all spammy strings\n",
    "    spammy_strings = []\n",
    "    \n",
    "    for training_example in arr:\n",
    "        #if message is SPAM\n",
    "        if training_example[1]=='1':\n",
    "            #store this string in a list to make it easier to count later\n",
    "            spammy_strings.append(training_example[0])\n",
    "            \n",
    "            training_string = training_example[0] #get the string\n",
    "            words = training_string.split() #split into words\n",
    "    \n",
    "    #convert spammystrings into one big string \n",
    "    spam_as_str = ''\n",
    "    for element in spammy_strings:\n",
    "        spam_as_str = spam_as_str + element\n",
    "        \n",
    "    #print (spam_as_str)\n",
    "        \n",
    "    #initialize counts as all unique spam words + all zeros\n",
    "    counts = np.zeros(len(all_spam_words), dtype=np.int8)\n",
    "    #counts = np.column_stack((all_spam_words, zer)) #dont need this???\n",
    "    #print (all_spam_words,counts)\n",
    "    \n",
    "    \n",
    "    # go through big string of spam, count each occurrence of each word in all_spam_words\n",
    "    # add this occurence to counts\n",
    "    \n",
    "    spammy_words = [] #list of every word that appears in every spam \n",
    "    \n",
    "    for i in spammy_strings:\n",
    "        str_split = i.split()\n",
    "        for j in str_split:\n",
    "            spammy_words.append(j)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for word in spammy_words:\n",
    "        pos = all_spam_words.index(word) #pos of word in both all_spam_words AND counts\n",
    "        #print(pos)\n",
    "        counts[pos] = counts[pos] + 1\n",
    "    \n",
    "    print (counts)\n",
    "    dic = counts[:dict_size], \n",
    "    return dic\n",
    "    \n",
    "\n",
    "    \n",
    "training_set = process_text('Training.csv')\n",
    "dictionary = create_dict(training_set, 150) #150 words\n",
    "\n",
    "#make feature vectors for each comment, store in list, dictsize x no. of features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b4c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
